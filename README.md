# Матчинг описания вакансий и резюме

Наш проект посвящён созданию поисковой системы, которая по входному резюме находит наиболее подходящие варианты вакансий из сохранённых в базе.

## Постановка бизнес-задачи

**Кому и как это может полезным?**

- улучшение поиска вакансий соискателями,
- более оптимальная рекомендация вакансий на соответствующих платформах.

## ML-решение

### Общий пайплайн

Для вакансий и резюме строятся их векторные представления одинакой размерности на основе наиболее важных текстовых признаков.

Для ускорения процесса поиска сохранённые вакансии предварительно кластеризуются, поэтому далее вместо того, чтобы сравнивать входное резюме со всеми вакансиями, мы будем находить ближайший к эмбеддингу входного резюме кластер с вакансиями и в качестве результатов поиска будем выдавать несколько примеров вакансий из этого кластера.

### Используемые данные

В качестве данных мы использовали:

1. готовый датасет с вакансиями с сайта "Работа России" (датасет №3 по ссылке: [datasets](https://trudvsem.ru/opendata/datasets)): для формирования базы вакансий использовали только те вакансии, в которых заполнены поля с навыками и требованиями по знанию языков. Из этих вакансий в итоге было взято 1000 примеров.
2. собранный нами (с помощью API hh.ru) датасет резюме. Опишем подробнее процесс подготовки датасета:
    При подготовке датасета вакансий и резюме из hh.ru команда столкнулась с проблемой парсинга информации. Если, для поиска вакансий, сервис предоставляет бесплатный API, то, для сбора данных о резюме, необходимо иметь аккаунт работодателя и оплатить доступ к платному API резюме. Поэтому, для сбора данных, был взят за основу [репозиторий](https://github.com/kate-red/HH_parcer), подготовленный для поиска информации об актуальных резюме, и доработан для нужд команды. Алгоритм, используемый в данном примере, скачивает HTML страницы, с актуальным списком резюме, для локального доступа к ним. После, в коде данных страниц ищутся специализированные id резюме, с помощью которых осуществляется их актуализация и хранение в сервисе hh.ru . Затем, с помощью этих индивидуальных номеров, идёт обращение к сервису для скачивания HTML-страниц, непосредственно, самих резюме. И только потом, происходит скраппинг информации о каждом человеке, оставившем свою информацию на сервисе hh.ru. Таким образом, собраная информация интерпритируется в CSV-файл, хранящий в себе всю необходимую информацию: id, название резюме, город проживания человека, возраст, пол, род занятий, желаемый заработок, опыт работы, уровень образования, владение языками, полезные навыки, информация о себе.
    - [Ссылка](https://github.com/DL-teammm/matching_resumes_with_vacancies/tree/main/Datasets) на собранный датасет резюме
    - [Ссылка](https://github.com/DL-teammm/matching_resumes_with_vacancies/tree/main/notebooks/scraping_datraset) на код
   <div class="row" style="display: flex;">
      <div class="column" style="display: flex; padding: 5px;">
            <img inline src="./imgs/dataset.jpg" alt="собранный датасет" width="800"/>
      </div>
    </div>

### Проведённые эксперименты

Были реализованы несколько подходов для построения эмбеддингов резюме и вакансий:

1. TF-IDF:
    - из данных были выделены наиболее важные столбцы (должность, навыки, знание языков, город и т.п.);
    - текстовые данные были предобработаны: удалены стоп-слова, все токены были приведены к начальной форме и т.д.;
    - по каждому из выделенных столбцов были посчитаны tf-idf коэффициенты и построены эмбеддинги;
    - в качестве итогового эмбеддинга вакансии или резюме была взята конкатенация полученных эмбеддингов по отдельных столбцам.

    К сожалению, этот подход сработал не очень хорошо на наших данных: при дальнейшей кластеризации построенных эмбеддингов вакансий внутри кластеров было много примеров разрозненных, непохожих профессий.
  
2. FastText: ...
3. ruBERT: ...

Для каждого из этих подходов была проведена кластеризация векторов вакансий при помощи метода K-Means.

Для сравнения качества построенных разными подходами эмбеддингов были построены графики метрик качества (при варьировании параметра k -- числа классов) для задачи кластеризации: SSE и Davies-Bouldin Index. Полученные графики представлены ниже:

1. TF-IDF:
    <div class="row" style="display: flex;">
      <div class="column" style="display: flex; padding: 5px;">
            <img inline src="./imgs/k_sse_tf_idf.png" alt="SSE, TF-IDF" width="800"/>
      </div>
    </div>

    <div class="row" style="display: flex;">
      <div class="column" style="display: flex; padding: 5px;">
            <img inline src="./imgs/k_db_index_tf_idf.png" alt="DB Index, TF-IDF" width="800"/>
      </div>
    </div>

...

Ноутбуки с проведёнными экспериментами лежат в папке ```notebooks```.

### Итоговый выбор подхода

В качестве финального подхода для построения эмбеддингов мы выбрали FastText, поскольку при кластеризации полученных с его помощью эмбеддингов вакансий результаты выглядели наиболее оптимальными: в кластерах были сосредоточены похожие друг на друга профессии, в разных кластерах профессии отличались друг от друга. Данную качественную оценку также подтверждают построенные оценки качества SSE и Davies-Bouldin Index.

## Характеристики нашего решения

Метрики качества кластеризации на построенных эмбеддингах вакансий: SSE = 12.17, DB Index = 2.02.

Замеры скорости построения эмбеддинга по входному резюме: ...

Замеры скорости поиска подходящих вакансий из сохранённой базы: ...

## Веб-сервис

Для демонстрации работы приложения был реализован веб-сервис на streamlit: ... Его можно запустить локально, загрузить резюме в формате hh.ru и посмотреть, какие вакансии из сохранённой базы подбирает наше решение.

### Демо

Демо с работой веб-сервиса можно посмотреть здесь: ...
